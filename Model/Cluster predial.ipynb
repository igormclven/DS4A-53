{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "2.7.5",
      "name": "python",
      "pygments_lexer": "ipython2",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    },
    "name": "Clustering predial",
    "modifiedBy": "analitica_bienesar"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clustering predial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notebook automatically generated from your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model KMeans (k\u003d4), trained on 2021-08-25 04:10:56."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generated on 2021-08-28 17:08:16.182745"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clustering\nThis notebook will reproduce the steps for clustering the dataset predial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Warning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal of this notebook is to provide an easily readable and explainable code that reproduces the main steps\nof training the model. It is not complete: some of the preprocessing done by the DSS visual machine learning is not\nreplicated in this notebook. This notebook will not give the same results and model performance as the DSS visual machine\nlearning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s start with importing the required libs :"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\nimport dataiku\nimport numpy as np\nimport pandas as pd\nimport sklearn as sk\nimport dataiku.core.pandasutils as pdu\nfrom dataiku.doctor.preprocessing import PCA\nfrom collections import defaultdict, Counter"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And tune pandas display options:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.set_option(\u0027display.width\u0027, 3000)\npd.set_option(\u0027display.max_rows\u0027, 200)\npd.set_option(\u0027display.max_columns\u0027, 200)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Importing base data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first step is to get our machine learning dataset:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We apply the preparation that you defined. You should not modify this.\npreparation_steps \u003d []\npreparation_output_schema \u003d {u\u0027userModified\u0027: False, u\u0027columns\u0027: [{u\u0027type\u0027: u\u0027bigint\u0027, u\u0027name\u0027: u\u0027col_0\u0027}, {u\u0027type\u0027: u\u0027bigint\u0027, u\u0027name\u0027: u\u0027consecutivo predial\u0027}, {u\u0027type\u0027: u\u0027bigint\u0027, u\u0027name\u0027: u\u0027corregimiento\u0027}, {u\u0027type\u0027: u\u0027bigint\u0027, u\u0027name\u0027: u\u0027barrio\u0027}, {u\u0027type\u0027: u\u0027bigint\u0027, u\u0027name\u0027: u\u0027manzana\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027cc_tarifa\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027estrato\u0027}, {u\u0027type\u0027: u\u0027bigint\u0027, u\u0027name\u0027: u\u0027clase\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027area_terr\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027area_terr_comun\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027area_const\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027area_const_comun\u0027}, {u\u0027type\u0027: u\u0027bigint\u0027, u\u0027name\u0027: u\u0027destinacion\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027vlr_terr\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027vlr_terr_comun\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027vlr_const\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027vlr_const_comun\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027vlr_tot_avaluo\u0027}, {u\u0027type\u0027: u\u0027bigint\u0027, u\u0027name\u0027: u\u0027total_owners\u0027}, {u\u0027type\u0027: u\u0027bigint\u0027, u\u0027name\u0027: u\u0027year\u0027}, {u\u0027type\u0027: u\u0027double\u0027, u\u0027name\u0027: u\u0027invoice\u0027}]}\n\nml_dataset_handle \u003d dataiku.Dataset(\u0027predial\u0027)\nml_dataset_handle.set_preparation_steps(preparation_steps, preparation_output_schema)\n%time ml_dataset \u003d ml_dataset_handle.get_dataframe(limit \u003d 100000)\n\nprint (\u0027Base data has %i rows and %i columns\u0027 % (ml_dataset.shape[0], ml_dataset.shape[1]))\n# Five first records\",\nml_dataset.head(5)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initial data management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The preprocessing aims at making the dataset compatible with modeling.\nAt the end of this step, we will have a matrix of float numbers, with no missing values.\nWe\u0027ll use the features and the preprocessing steps defined in Models.\n\nLet\u0027s only keep selected features"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_dataset \u003d ml_dataset[[u\u0027total_owners\u0027, u\u0027estrato\u0027, u\u0027destinacion\u0027, u\u0027manzana\u0027, u\u0027area_terr\u0027, u\u0027cc_tarifa\u0027, u\u0027invoice\u0027, u\u0027area_const\u0027, u\u0027vlr_tot_avaluo\u0027, u\u0027vlr_terr\u0027, u\u0027barrio\u0027, u\u0027vlr_const\u0027]]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s first coerce categorical columns into unicode, numerical features into floats."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# astype(\u0027unicode\u0027) does not work as expected\n\ndef coerce_to_unicode(x):\n    if sys.version_info \u003c (3, 0):\n        if isinstance(x, str):\n            return unicode(x,\u0027utf-8\u0027)\n        else:\n            return unicode(x)\n    else:\n        return str(x)\n\n\ncategorical_features \u003d [u\u0027estrato\u0027, u\u0027destinacion\u0027, u\u0027manzana\u0027, u\u0027barrio\u0027]\nnumerical_features \u003d [u\u0027total_owners\u0027, u\u0027area_terr\u0027, u\u0027cc_tarifa\u0027, u\u0027invoice\u0027, u\u0027area_const\u0027, u\u0027vlr_tot_avaluo\u0027, u\u0027vlr_terr\u0027, u\u0027vlr_const\u0027]\ntext_features \u003d []\nfrom dataiku.doctor.utils import datetime_to_epoch\nfor feature in categorical_features:\n    ml_dataset[feature] \u003d ml_dataset[feature].apply(coerce_to_unicode)\nfor feature in text_features:\n    ml_dataset[feature] \u003d ml_dataset[feature].apply(coerce_to_unicode)\nfor feature in numerical_features:\n    if ml_dataset[feature].dtype \u003d\u003d np.dtype(\u0027M8[ns]\u0027) or (hasattr(ml_dataset[feature].dtype, \u0027base\u0027) and ml_dataset[feature].dtype.base \u003d\u003d np.dtype(\u0027M8[ns]\u0027)):\n        ml_dataset[feature] \u003d datetime_to_epoch(ml_dataset[feature])\n    else:\n        ml_dataset[feature] \u003d ml_dataset[feature].astype(\u0027double\u0027)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s copy our dataset to keep it for eventual profiling at the end."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# train dataset will be the one on which we will apply ml technics\ntrain \u003d ml_dataset.copy()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Features preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first thing to do at the features level is to handle the missing values.\nLet\u0027s reuse the settings defined in the model"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "drop_rows_when_missing \u003d []\nimpute_when_missing \u003d [{\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027total_owners\u0027}, {\u0027impute_with\u0027: u\u0027MODE\u0027, \u0027feature\u0027: u\u0027estrato\u0027}, {\u0027impute_with\u0027: u\u0027MODE\u0027, \u0027feature\u0027: u\u0027destinacion\u0027}, {\u0027impute_with\u0027: u\u0027MODE\u0027, \u0027feature\u0027: u\u0027manzana\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027area_terr\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027cc_tarifa\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027invoice\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027area_const\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027vlr_tot_avaluo\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027vlr_terr\u0027}, {\u0027impute_with\u0027: u\u0027MODE\u0027, \u0027feature\u0027: u\u0027barrio\u0027}, {\u0027impute_with\u0027: u\u0027MEAN\u0027, \u0027feature\u0027: u\u0027vlr_const\u0027}]\n\n# Features for which we drop rows with missing values\"\nfor feature in drop_rows_when_missing:\n    train \u003d train[train[feature].notnull()]\n    \n    print (\u0027Dropped missing records in %s\u0027 % feature)\n\n# Features for which we impute missing values\"\nfor feature in impute_when_missing:\n    if feature[\u0027impute_with\u0027] \u003d\u003d \u0027MEAN\u0027:\n        v \u003d train[feature[\u0027feature\u0027]].mean()\n    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027MEDIAN\u0027:\n        v \u003d train[feature[\u0027feature\u0027]].median()\n    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027CREATE_CATEGORY\u0027:\n        v \u003d \u0027NULL_CATEGORY\u0027\n    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027MODE\u0027:\n        v \u003d train[feature[\u0027feature\u0027]].value_counts().index[0]\n    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027CONSTANT\u0027:\n        v \u003d feature[\u0027value\u0027]\n    train[feature[\u0027feature\u0027]] \u003d train[feature[\u0027feature\u0027]].fillna(v)\n    \n    print (\u0027Imputed missing values in feature %s with value %s\u0027 % (feature[\u0027feature\u0027], coerce_to_unicode(v)))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now handle the categorical features (still using the settings defined in Models):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s dummy-encode the following features.\nA binary column is created for each of the 100 most frequent values."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "LIMIT_DUMMIES \u003d 100\n\ncategorical_to_dummy_encode \u003d [u\u0027estrato\u0027, u\u0027destinacion\u0027, u\u0027manzana\u0027, u\u0027barrio\u0027]\n\n# Only keep the top 100 values\ndef select_dummy_values(train, features):\n    dummy_values \u003d {}\n    for feature in categorical_to_dummy_encode:\n        values \u003d [\n            value\n            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)\n        ]\n        dummy_values[feature] \u003d values\n    return dummy_values\n\nDUMMY_VALUES \u003d select_dummy_values(train, categorical_to_dummy_encode)\n\ndef dummy_encode_dataframe(df):\n    for (feature, dummy_values) in DUMMY_VALUES.items():\n        for dummy_value in dummy_values:\n            dummy_name \u003d u\u0027%s_value_%s\u0027 % (feature, coerce_to_unicode(dummy_value))\n            df[dummy_name] \u003d (df[feature] \u003d\u003d dummy_value).astype(float)\n        del df[feature]\n        print (\u0027Dummy-encoded feature %s\u0027 % feature)\n\ndummy_encode_dataframe(train)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s rescale numerical features"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rescale_features \u003d {u\u0027total_owners\u0027: u\u0027AVGSTD\u0027, u\u0027vlr_terr\u0027: u\u0027AVGSTD\u0027, u\u0027area_terr\u0027: u\u0027AVGSTD\u0027, u\u0027cc_tarifa\u0027: u\u0027AVGSTD\u0027, u\u0027invoice\u0027: u\u0027AVGSTD\u0027, u\u0027area_const\u0027: u\u0027AVGSTD\u0027, u\u0027vlr_tot_avaluo\u0027: u\u0027AVGSTD\u0027, u\u0027vlr_const\u0027: u\u0027AVGSTD\u0027}\nfor (feature_name, rescale_method) in rescale_features.items():\n    if rescale_method \u003d\u003d \u0027MINMAX\u0027:\n        _min \u003d train[feature_name].min()\n        _max \u003d train[feature_name].max()\n        scale \u003d _max - _min\n        shift \u003d _min\n    else:\n        shift \u003d train[feature_name].mean()\n        scale \u003d train[feature_name].std()\n    if scale \u003d\u003d 0.:\n        del train[feature_name]\n        \n        print (\u0027Feature %s was dropped because it has no variance\u0027 % feature_name)\n    else:\n        print (\u0027Rescaled %s\u0027 % feature_name)\n        train[feature_name] \u003d (train[feature_name] - shift).astype(np.float64) / scale"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Removing outliers"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Remove outliers from train set\nfrom dataiku.doctor.preprocessing.dataframe_preprocessing import detect_outliers\n\noutliers \u003d detect_outliers(train, 0.9, 100, 0.01)\ntrain \u003d train[~outliers]\n\nprint (\"%s outliers found\" % (outliers.sum()))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Modeling"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.cluster import KMeans\nclustering_model \u003d KMeans(n_clusters\u003d4)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can finally cluster our dataset!"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%time clusters \u003d clustering_model.fit_predict(train)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build up our result dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inertia"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print (clustering_model.inertia_)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Silhouette"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import silhouette_score\nsilhouette \u003d silhouette_score(train.values, clusters, metric\u003d\u0027euclidean\u0027, sample_size\u003d2000)\nprint (\"Silhouette score :\", silhouette)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Join our original dataset with the cluster labels we found."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "final \u003d train.join(pd.Series(clusters, index\u003dtrain.index, name\u003d\u0027cluster\u0027))\nfinal[\u0027cluster\u0027] \u003d final[\u0027cluster\u0027].map(lambda cluster_id: \u0027cluster\u0027 + str(cluster_id))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the cluster sizes"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "size \u003d pd.DataFrame({\u0027size\u0027: final[\u0027cluster\u0027].value_counts()})\nsize.head()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Draw a nice scatter plot"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "axis_x \u003d train.columns[0]   # change me\naxis_y \u003d train.columns[1]  # change me\n\nfrom ggplot import ggplot, aes, geom_point\nprint(ggplot(aes(axis_x, axis_y, colour\u003d\u0027cluster\u0027), final) + geom_point())"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That\u0027s it. It\u0027s now up to you to tune your preprocessing, your algo, and your analysis !\n"
      ]
    }
  ]
}